{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I = \n",
      " tensor([[1, 0],\n",
      "        [0, 1]])\n",
      "I.ndimension() =  2\n",
      "I.shape =  torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "I = torch.tensor([[1,0], [0,1]])\n",
    "print('I = \\n', I)\n",
    "print('I.ndimension() = ', I.ndimension())\n",
    "print('I.shape = ', I.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [0., 1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I1 = torch.eye(2)   # 单位阵\n",
    "I1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.3184,  0.6487,  0.7557, -0.6787],\n",
      "        [-0.6118, -0.5305, -0.6568, -0.0726],\n",
      "        [ 0.3631, -0.7684,  1.5212,  0.5558],\n",
      "        [ 0.1419,  0.2456, -0.5595,  1.0284]])\n",
      "tensor([-0.6118, -0.5305, -0.6568, -0.0726])\n",
      "tensor([[-0.6118, -0.5305],\n",
      "        [ 0.3631, -0.7684]])\n"
     ]
    }
   ],
   "source": [
    "z = torch.randn((4,4))\n",
    "print(z)\n",
    "print(z[1, :])\n",
    "print(z[1:3, :2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 0.],\n",
       "         [0., 1.]]),\n",
       " tensor([[ 1.,  0.],\n",
       "         [ 0., -1.]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pauli_z = I1.clone()\n",
    "pauli_z[1,1] =  -1\n",
    "I1, pauli_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 0.],\n",
       "         [0., 1.]]),\n",
       " tensor([[ 1.,  0.],\n",
       "         [ 0., -1.]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = I1 * 1\n",
    "K[1,1] = -1\n",
    "I1, K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.,  0.],\n",
       "         [ 0., -1.]]),\n",
       " tensor([[ 1.,  0.],\n",
       "         [ 0., -1.]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I1的值改变了，利用等号赋值并没有创建新内存\n",
    "J = I1\n",
    "J[1,1] = -1\n",
    "I1, J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 3, 5, 7]),\n",
       " tensor([[1, 3],\n",
       "         [5, 7]]),\n",
       " tensor([[1, 3],\n",
       "         [5, 7]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1,3,5,7])\n",
    "x1 = torch.reshape(x, [2,2])\n",
    "x1_ = x.reshape(2,2)\n",
    "x, x1, x1_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 5],\n",
       "         [3, 7]]),\n",
       " tensor([[1, 5],\n",
       "         [3, 7]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = x1.permute(1, 0)\n",
    "x2_ = x1.t()\n",
    "x2, x2_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n",
      "torch.Size([24])\n",
      "flatten与reshape向量化所得结果的差别 0.0\n",
      "矩阵化T[0,1]的形状为 torch.Size([6, 4])\n"
     ]
    }
   ],
   "source": [
    "T = torch.randn(2,3,4)\n",
    "print(T.shape)\n",
    "V = T.flatten()\n",
    "print(V.shape)\n",
    "print('flatten与reshape向量化所得结果的差别', (V-T.reshape(-1)).norm().item())\n",
    "print('矩阵化T[0,1]的形状为', T.reshape(-1,4).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u*c= tensor([2, 4, 6, 8])\n",
      "u*d= tensor([2, 4, 6, 8])\n",
      "u+c= tensor([3, 4, 5, 6])\n",
      "u+v= tensor([ 3,  6,  9, 12])\n",
      "u*v= tensor([ 2,  8, 18, 32])\n"
     ]
    }
   ],
   "source": [
    "u = torch.tensor([1,2,3,4])\n",
    "v = torch.tensor([2,4,6,8])\n",
    "c = 2\n",
    "d = torch.tensor([2])\n",
    "\n",
    "print('u*c=', u*c)\n",
    "print('u*d=', u*d)\n",
    "print('u+c=', u+c)\n",
    "print('u+v=', u+v)\n",
    "print('u*v=', u*v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "向量外积outer= \n",
      " tensor([[ 2,  4,  6,  8],\n",
      "        [ 4,  8, 12, 16],\n",
      "        [ 6, 12, 18, 24],\n",
      "        [ 8, 16, 24, 32]])\n",
      "向量外积einsum= \n",
      " tensor([[ 2,  4,  6,  8],\n",
      "        [ 4,  8, 12, 16],\n",
      "        [ 6, 12, 18, 24],\n",
      "        [ 8, 16, 24, 32]])\n"
     ]
    }
   ],
   "source": [
    "u = torch.tensor([1,2,3,4])\n",
    "v = torch.tensor([2,4,6,8])\n",
    "print('向量外积outer= \\n', u.outer(v))\n",
    "print('向量外积einsum= \\n', torch.einsum('a,b->ab', u, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12])\n",
      "torch.Size([3, 4])\n",
      "tensor([999,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11])\n"
     ]
    }
   ],
   "source": [
    "# reshape, view, permute\n",
    "\n",
    "# reshape 重新调整张量的形状，但可能会复制数据（如果原始数据在内存中不是连续的）。\n",
    "# 创建一个连续的张量\n",
    "x = torch.arange(12)\n",
    "print(x.shape)  # torch.Size([12])\n",
    "\n",
    "# 使用 reshape 改变形状\n",
    "y = x.reshape(3, 4)\n",
    "print(y.shape)  # torch.Size([3, 4])\n",
    "\n",
    "# 修改 y 的值，x 也会同步变化（说明没有复制数据，原始数据在内存中是连续的）\n",
    "y[0, 0] = 999\n",
    "print(x)  # x[0] 变成了 999\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "tensor([999,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(12)\n",
    "y = x.view(3, 4)\n",
    "print(y.shape)  # torch.Size([3, 4])\n",
    "\n",
    "# 修改 y 的值，x 也会同步变化（说明没有复制数据）\n",
    "y[0, 0] = 999\n",
    "print(x)  # x[0] 变成了 999\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m      3\u001b[0m y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# 交换维度\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# ❌ 这里会报错 RuntimeError: view size is not compatible with input tensor\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "# 如果数据内存不连续，view会报错\n",
    "x = torch.randn(2, 3, 4)\n",
    "y = x.permute(1, 0, 2)  # 交换维度\n",
    "z = y.view(3, 8)  # ❌ 这里会报错 RuntimeError: view size is not compatible with input tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3316,  0.4163,  0.8924, -2.4649, -1.9064,  1.7005, -1.0033, -2.6787],\n",
       "        [-0.8830,  2.5085, -1.4781,  0.7459, -0.4559,  0.1434,  1.1997, -0.2149],\n",
       "        [-0.7904, -1.2990,  1.5640,  1.2060, -0.7360, -0.6100, -0.1774,  0.5740]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = y.contiguous().view(3, 8)  # 先变成连续的\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n",
      "tensor([[[-0.7190, -1.8848, -0.1372,  0.8433],\n",
      "         [ 0.7617,  0.4837,  0.4637, -1.2413],\n",
      "         [ 2.1244, -0.3036, -0.2629,  0.1694]],\n",
      "\n",
      "        [[-0.3058,  1.5784,  2.1918, -0.8442],\n",
      "         [ 1.4226, -0.4437,  1.7754, -0.0585],\n",
      "         [-0.0236,  0.7223, -0.2508,  0.5784]]])\n",
      "torch.Size([3, 2, 4])\n",
      "tensor([[[-0.7190, -1.8848, -0.1372,  0.8433],\n",
      "         [-0.3058,  1.5784,  2.1918, -0.8442]],\n",
      "\n",
      "        [[ 0.7617,  0.4837,  0.4637, -1.2413],\n",
      "         [ 1.4226, -0.4437,  1.7754, -0.0585]],\n",
      "\n",
      "        [[ 2.1244, -0.3036, -0.2629,  0.1694],\n",
      "         [-0.0236,  0.7223, -0.2508,  0.5784]]])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 4)  # (batch, channels, height)\n",
    "print(x.shape)  # torch.Size([2, 3, 4])\n",
    "print(x)\n",
    "\n",
    "y = x.permute(1, 0, 2)  # 交换第 0 维和第 1 维\n",
    "print(y.shape)  # torch.Size([3, 2, 4])\n",
    "print(y)\n",
    "\n",
    "print(y.is_contiguous())  # False\n",
    "z = y.contiguous().view(3, 8)  # 先变成连续的，再 reshape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
